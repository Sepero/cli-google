from scrapy.spider import BaseSpider
from scrapy.http import Request
from bs4 import BeautifulSoup
import time, urllib

class GoogleParser(BaseSpider):

    name = "googleparser"
    
    allowed_urls = ["google.com"]

    max_results = ""
    count = 0
    query = ""
    _url = "http://www.google.com/search?q="
    
    def start_requests():
        if not self.query:
            self.query = urllib.quote(raw_input("What would you like to search Google for?\n"))
            self.max_results = str(input("How many results would you like before the spider stops?"))
            self._url += self.query + "&start=" + self.count
        
        return [Request(url=self._url)]

    def parse(self, response):
        if self.count >= self.max_results:
            exit
        else:
            self.count += 10

        soup = BeautifulSoup(response)
        
        for link in soup('a'):
            if "/url?=" in result['href']:
                print str(result['href'].split('&')[0][7:])
            
        
        yield Request(url=self._url)
